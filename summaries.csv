mmzRYGCfTzc,"hi everyone this is another video on the
attention mechanism series brought to
you by the MLS studio in this video I
will talk about multi-head attention
that was proposed in the paper attention
is all you need
here is the outline of this video in the
last video I described the scaled dot
product attention in full detail but
given that the scale.product or sdp is
at the core of multi-head attention so
in this video first we will see a quick
recap of sdp then I will describe what
multi-head attention is and how it works
and finally we can see two ways of using
the attention mechanism which is either
using it as self-attention or using it
for cross attention so we will see the
differences between the two mechanisms
and at the end I will finish this video
with an exercise
so first let's start with a quick
overview of scale dot product attention
or sdp
given a sequence of words for example
words from an English sentence the goal
of sdp is to find the relationship
between these words
so here each rectangle represents a word
or token in this sentence so first we
extract features X1 to x t where T is
the sequence length
then from each x i we compute three
vectors q k and V which are known as
query key and value vectors these
computations are based on a matrix
multiplication as shown in the equation
box here
then we put these q k and V vectors
together to form matrices q k and v as
shown here at this point we are ready to
move to the first step in the
scale.product attention
on the left panel you can see the full
diagram of scale dot product attention
with q k and V matrices as input in the
first step we compute a DOT product or a
matrix multiplication between q and K
and we'll get a compatibility Matrix
which has dimensionality t by T
in the next step we scale the
compatibility matrix by 1 over a square
root of d sub K where d sub K is the
dimensionality of vectors q and K in the
previous video I explained why this
scaling is necessary but for the sake of
time we can skip this discussion here
step 3 is an optional step so I did not
cover that in the previous video
this step is only needed in some
applications such as in sequence to
sequence translation or Auto regressive
sequence generation
for example when we are training a model
to generate new sequences and our input
sequences contain past and future
sections of a sentence the objective is
to predict future tokens of that input
sequence but the input sequence contains
future tokens so in that case we have to
mask the future tokens
and we can do that by setting the upper
triangular section of the compatibility
Matrix to negative Infinity as shown
here this will ensure that word I will
not be able to see or attend towards
with larger index than itself
now in Step 4 we apply the softmax
function to normalize the compatibility
Matrix which results in the attention
weights
note that if we have masked the upper
triangle of the compatibility Matrix in
step 3 with negative Infinity then after
applying the soft Max the upper triangle
of the result will be zero and finally
in a step 5 we perform another matrix
multiplication between the attention
weights from previous step and the
Matrix V the result of this is our
context Matrix which we call Matrix Z so
now we can move on to multi-head
attention the idea of multi-ed attention
is as follows instead of performing a
single attention on large matrices q k
and V it is actually better to break it
into multiple smaller dimensions and
perform a scale dot product separately
on each of those smaller matrices this
is the full diagram of multi-head
attention proposed in the paper
attention is all you need so let's walk
through the steps involved in multi-head
attention
first we have to Define how many heads
we want to use for multi-head attention
the lowercase symbol H is used to
indicate the number of heads and
typically this value is set to 8. after
that we have to get that many number of
sets of q k and V matrices
for query matrices I'm showing them with
superscript 1 to Edge
so we have q1 to qh and we have K1 to KH
for the key matrices and V1 to VH for
the value matrices to get these matrices
we multiply x h times with h different
weight matrices for Q and the same thing
for K and for V
then in the second step we perform a
scaled dot product attention on each
triple set of q i k i and VI and we name
the output of each attention as the
context Matrix head I performing sdp on
all sets from 1 to Edge gives us head 1
to head h
for step 3 we have these H different
matrices head 1 to head Edge as shown
here
and each one of them has dimensionality
t by D over h
in this step we concatenate these heads
together and we get Matrix Z with
dimensionality t by D
finally in the last step we perform a
matrix multiplication between Matrix Z
and the learnable weight Matrix w o
resulting in the context output for this
multi-head attention layer a very
important Point here is that the
computational cost of multi-head
attention is more or less similar to
performing a single scale dot product on
large q k and V matrices but while the
computational cost is similar multi-ed
attention is more beneficial than a
single scale dot product attention the
reason is multi-ed attention can extract
context information from different
subspaces at different positions of the
input sequence
so that brings us to the last topic of
this video self-attention versus cross
attention so self-attention is when we
want to get the relationship among the
words in an input sequence and we have
seen several examples so far in this
video and the previous one each word in
this sequence can attend to other words
with different degrees
but for cross-attention we have two
sequences X and Y and this could be for
example a translation application like X
is the sentences in English and Y is
their translations in German and the
length of sequence is X and Y could be
different here we can use cross
attention to make each word in sequence
y to attend towards in sequence X so
let's see how this can be done
so as we have seen so far for
self-attention with scale.product we
extract matrices q k and V from the
input sequence X but for cross attention
we have two input sequences X and Y and
in order to make sequence y attend to
the words in sequence X we extract
Matrix Q from y abstract Matrix K and V
from X therefore the main difference
between the self-attention and cross
attention is basically where q k and V
come from after we obtain these q k and
V the rest are the same
I want to finish this video with an
exercise so imagine we have x with
dimensionality T1 by D
and Y with dimensionality T2 by D
and we want to know what will be the
dimensionality of these other matrices
that are computed for a single head
cross attention between X and Y where Y
is attending to X
so find the dimensionality of q k v the
compatibility Matrix q k transpose as
well as the final context Matrix Z I
will provide the answer in the
description of this video
thanks for watching","openai Error: 401 Incorrect API key provided: ssk-JaAp****************************************p21r. You can find your API key at https://platform.openai.com/account/api-keys.","The attention mechanism is either used for self-attention or for cross attention. In this video I will describe what multi-head attention is and how it works. We will also see two ways of using the attention mechanism. At the end I will finish this video with an exercise to test our knowledge of the mechanisms.","In this video I will be talking about multi-head attention.","this is another video on the attention mechanism series brought to you by the MLS studio in this video I will talk about multi-head attention that was proposed in the paper attention is all you need here is the outline of this video in the last video I described the scaled dot product attention in full detail but given that the scale.product or sdp is at the core of multi-headed attention so we will see the differences between the two mechanisms . at the end I will finish this video with an exercise so first let's start with","huggingface-ledlargebooksummarization Error: API call failed with status 400: Bad Request"
bpG3gqDM80w,"Thanks to Brilliant for helping support this episode.
Hmm, tensors.
Holo Clone!
What do mathematicians say about tensors?
A rank-n tensor in m-dimensions is a mathematical object that has n indices
and m-to-the-n components that obeys certain transformation rules.
Pfff! We can do better than that!
This episode was made possible by generous supporters on Patreon.
Hey Crazies.
If you’re like me, you find definitions in textbooks incredibly unsatisfying.
In their defense, definitions like this one are correct, complete, and concise.
We call them the three c’s.
(Off Camera) No one calls them that!
I call them that!
Anyway, correctness is absolutely vital.
It doesn’t matter how clear your explanation is if it’s wrong,
but the other two are very flexible, so let’s see what we can do.
By the end of the video, you should understand this definition with all its math speak.
To get there though, we’re going to need a little context because that [BEEP] is abstract as [BEEP].
The word ""tensor"" actually comes from an old Latin word meaning ""to stretch.""
If you pull an object outward along its length, it experiences something called ""tensile stress.""
In response, it’s length increases.
Which, you know, makes sense.
Except, that’s not the only type of stress an object can experience.
This cube could be stretched or compressed along any of the 3 spatial directions.
Wouldn’t a vector be enough for that?
First, a vector is a tensor and, second,
there are 6 other stresses I haven’t mentioned yet.
The cube could also be sheared along those directions.
That’s 9 possible stresses.
Yeah, but can’t we just add the forces together along each direction?
No. No we can’t.
Each of these forces makes the cube respond in a different way.
We have to consider them all separately.
These 9 different stresses are usually organized into a 3-by-3 matrix called the stress tensor.
Quick Disclaimer: It’s not a tensor because I can write it as a matrix.
Matrices and tensors are not the same thing.
A matrix is just a convenient way to organize numbers
sometimes.
Writing the stress tensor like this, we can see it clearly has 9 components,
but our definition mentioned two specific properties:
Rank and Dimension.
This cube is 3-dimensional, so any tensor describing it’s behavior will also be dimension-3.
That’s why our stress tensor is organized into 3 rows and 3 columns.
Each corresponds to a specific direction in 3-dimensional space.
Seriously. It’s that easy.
Rank is the amount of information you need to find a specific component.
In this case, we only need a row and a column.
That’s 2 pieces of information, so we say the tensor is rank-2.
The stress tensor is rank-2 and dimension-3.
Matrix notation is really convenient for rank-2 tensors of any dimension.
With the electromagnetic field tensor, you still only need a row and column to find a component,
so it’s rank-2.
However, there 4 rows and 4 columns, which means this tensor is 4-dimensional.
The electromagnetic field tensor is rank-2 and dimension-4.
This notation starts to fall apart with higher rank tensors though.
For example, a rank-3 tensor requires 3 pieces of information to find a component.
While this is still technically a matrix, the math operations aren’t very obvious.
It gets even worse with rank-4 tensors.
This is interesting looking, but it's not very useful.
Honestly, the matrix notation is kind of like a security blanket anyway.
It’s only there to make people feel more comfortable when they’re first learning about tensors.
Then what are we supposed to use?
Index notation!
Rank-zero means you don’t need any information to find a component.
That’s just a scalar.
Boring!
Rank-1 means we only need one piece of information to find a component.
In other words, we only need one index.
That’s just a vector.
Maybe something like the velocity of a ball across a table.
Rank-2 means we need two pieces of information or 2 indices.
Traditionally, we use Latin letters for 2 and 3 dimensions and Greek letters for 4 dimensions,
which helps make rank and dimension more obvious at a glance.
Rank-3 means 3 indices, rank-4 means 4 indices, and so on.
Ok fine, but what makes them tensors?
How they transform!
Humans have a decent intuition about velocity, so let’s start there.
This ball could encounter some wind, which would slow it down,
but that’s not the kind of transformation we’re talking about.
We’re not talking about how the situation might change.
We’re talking about how our coordinate system might change.
To use physics on this scenario, we need to assign a coordinate system to it.
Something like this.
That’s just a tool though.
We could just as easily have put the coordinates over here,
or over here,
or even over here.
We could have rotated them like this,
or like this.
We could have even stretched or compressed either of the axes.
But our choice of coordinates should have no effect on physical reality.
None of these transformations will change the velocity of the ball.
Wait, wouldn’t a rotation change the direction?
No, it’s still moving to the right.
But don’t the components change?
Yes, but that’s just how we’re representing it, not what it actually is.
This vector is a rank-1 dimension-2 tensor.
It has two components, one for each of the dimensions.
Any change in coordinates will change the values of those components.
But the physical nature of that vector remains the same.
Wouldn’t any arrow do that?
Actually, no.
Take angular momentum for example.
If we put our coordinates at the center of this circular orbit,
the angular momentum points up.
It’s steady and constant.
But, if we shift the coordinates to the edge of the circle, that’s no longer the case.
The angular momentum changes over time.
It even goes to zero for a brief moment,
which is ridiculous!
That shouldn’t happen with a real physical thing,
so we call angular momentum a false vector or pseudovector.
It has a direction, so it masquerades as a vector, but it isn’t actually a vector.
Velocity is a real vector.
Angular momentum is not.
It’s a pseudovector.
If a real vector is zero in one set of coordinates, it must be zero in all of them.
No exceptions.
But doesn’t velocity go to zero if your coordinates move along with the moving thing?
Yes, but that’s not a 3-dimensional transformation.
It’s a 4-dimensional one.
Which means you can’t use 3-dimensional vectors.
This ball is moving relative to the table, but not relative to itself.
Shifting to a steadily moving coordinate system is something we call a boost
and it requires we include time as an additional axis.
This ball may be moving through space, but it’s also moving through time.
It has its own time axis.
We call this spacetime and a boost is a just a 4-dimenional coordinate rotation.
But, if we want to talk about velocity, we need a 4-dimensional velocity or 4-velocity,
which is a rank-1 dimension-4 tensor.
Don’t forget. This video is still about tensors.
This ball’s 4-velocity is a real vector.
It remains the same under these 4D rotations.
Just like regular 3-velocity did under 3D rotations.
You can’t work in 4-dimensions without using 4-dimensional tensors.
Come on crazies!
The same goes for things like the 3-dimensional magnetic field.
Moving electric charge will generate a magnetic field, but only if you see the charge moving.
If you’re moving along with it, the charge is stationary, which means no magnetic field.
The magnetic field is not a real vector.
It’s a pseudovector.
That’s why we came up with the rank-2 electromagnetic tensor.
It fixes this problem.
It’s a real tensor.
Unfortunately, a rank-2 tensor can’t be visualized as an arrow like a vector can,
but it can be understood as a transformation between vectors.
In fact, that’s exactly what this equation says about the EM tensor.
It transforms the 4-velocity of a charged particle into a force.
A moving charged particle inside of a force field experiences a force.
That's a little magical, isn't it?
Ok, let’s use the stress tensor instead.
Fine!
I’m a huge dice nerd. I love dice.
The best ones are the platonic solids.
Obviously.
Let’s consider the 4-sided one: The tetrahedron.
If we want to know the force on one of its surfaces, we just need to know its stress tensor.
Maybe one of its surfaces is facing this way.
If it’s experiencing stress described by this, then our surface is being nudged this way.
The area vector is transformed into a force vector.
So what’s a tensor?
It’s a number or collection of similar numbers that maintains its meaning under transformations.
If you make a different choice in coordinates, the components of the tensor will change,
but in a way that conspires to keep the meaning of the tensor the same.
This velocity vector is a rank-1 tensor that describes the motion of the ball,
regardless of the coordinate choice.
This stress tensor is a rank-2 tensor and describes how to get a force from area,
regardless of the coordinate choice.
If your number or collection of numbers doesn’t do that, then it’s not a tensor.
It’s a false tensor or pseudotensor.
Not being able to tell the difference can get you into some serious trouble,
at least in the math.
So got any questions about tensors?
Please ask in the comments.
If you're looking for a deeper dive, check out the book I wrote.
It’s got an entire chapter explaining tensors
and it’s available in paperback and as an eBook.
Thanks for liking and sharing this video.
Don’t forget to subscribe if you’d like to keep up with us.
And until next time, remember, it’s ok to be a little crazy.
If investing in your STEM skills is your kind of new year’s resolution, you should check out Brilliant.
Maybe you’re naturally curious or want to build your problem-solving skills
or need to develop confidence in your analytical abilities.
With Brilliant Premium, you can learn something new every day.
Brilliant’s thought-provoking math, science, and computer science content helps guide you to mastery.
by taking complex concepts and breaking them up into bite-sized understandable chunks.
There’s a whole course on linear algebra which is where matrices are important.
There’s even a course on 3D geometry where you can learn about platonic solids.
Brilliant helps you achieve your goals in STEM, one small commitment-to-learning at a time.
If this sounds like a service you’d like to use, go to brilliant dot org slash Science Asylum today.
The first 200 subscribers will get 20% off an annual subscription.
I really enjoyed some of the take-aways people got from my last video.
Like that the Earth has only been around the galaxy 20 times
or that the galaxy has only rotated somewhere between 50 and 60 times.
Most of us would have expected those numbers to be bigger, you know?
Anyway, thanks for watching.","openai Error: 401 Incorrect API key provided: ssk-JaAp****************************************p21r. You can find your API key at https://platform.openai.com/account/api-keys.","A rank-n tensor in m-dimensions is a mathematical object that has n indicesand m-to-the-n components that obeys certain transformation rules. By the end of the video, you should understand this definition with all its math speak. Thanks to Brilliant for helping support this episode.","In this week’s definition, we’re going to look at tensors.","This episode was made possible by generous supporters on Patreon . If you’re like me, you find definitions in textbooks incredibly unsatisfying . In their defense, definitions like this one are correct, complete, and concise .","huggingface-ledlargebooksummarization Error: API call failed with status 503: Service Unavailable"
